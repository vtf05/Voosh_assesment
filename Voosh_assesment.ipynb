{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Voosh_assesment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k7ECq1yQ1Zvi"
      },
      "outputs": [],
      "source": [
        "# Web_scraping.py\n",
        "\n",
        "from bs4 import BeautifulSoup as soup\n",
        "from urllib.request import urlopen\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(link):\n",
        "  get_url = link\n",
        "  content = urlopen(get_url)\n",
        "  page_soup = soup(content.read(),\"html.parser\")\n",
        "  content.close()\n",
        "\n",
        "  name = page_soup.find('h1',{\"class\":\"_3aqeL\"})\n",
        "  location = page_soup.find('div',{\"class\":\"Gf2NS _2Y6HW\"})\n",
        "  rating = page_soup.find('div',{\"class\":\"_2l3H5\"})\n",
        "  number_of_rating = page_soup.find('span',{\"class\":\"_1iYuU\"})\n",
        "  \n",
        "  Food_items=[]  #List to store name of the food items\n",
        "  prices=[]      #List to store prices of the food itmes\n",
        "  categories = []\n",
        "  descriptions = []\n",
        "  restaurent_name = []\n",
        "  locations = []\n",
        "  ratings = []\n",
        "  num_ratings = []\n",
        "  containers = page_soup.findAll(\"div\", attrs={\"class\":\"_2dS-v\"})\n",
        "  for x in containers :\n",
        "    category = x.find('h2',{\"class\":\"M_o7R _27PKo\"}) # getting food categories\n",
        "    mini_containers = x.findAll(\"div\",{\"class\":\"_2wg_t\"})\n",
        "    for j in mini_containers :\n",
        "      item = j.find(\"div\",{\"class\":\"styles_itemName__hLfgz\"}) # getting here food item names\n",
        "      price = j.find(\"span\",{\"class\":\"rupee\"}) # getting here food prices\n",
        "      item_des = j.find(\"div\",{\"class\":\"styles_itemDesc__3vhM0\"}) # getting description\n",
        "      \n",
        "      restaurent_name.append(name.text)\n",
        "      locations.append(location.text)\n",
        "      ratings.append(rating.text)\n",
        "      num_ratings.append(number_of_rating.text)\n",
        "      try :\n",
        "        categories.append(category.text)\n",
        "      except AttributeError :\n",
        "        categories.append(None)\n",
        "\n",
        "      try :\n",
        "        prices.append(price.text)\n",
        "      except AttributeError :\n",
        "        pass\n",
        "\n",
        "      try :\n",
        "        Food_items.append(item.text) \n",
        "      except AttributeError :\n",
        "        pass\n",
        "\n",
        "      try :\n",
        "        descriptions.append(item_des.text)\n",
        "      except AttributeError :\n",
        "        descriptions.append(None)\n",
        "  df = pd.DataFrame({'restaurent_name':restaurent_name,'location':locations,\n",
        "                     'ratings':ratings , 'number_of_rating' : num_ratings,\n",
        "                     'Food Item Name':Food_items,'item_prices':prices ,\n",
        "                     'item_description':descriptions,'category':categories          \n",
        "                     }) \n",
        "  return df        \n"
      ],
      "metadata": {
        "id": "dcuTSAB43y6N"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start Scrapping any Top 3 Restaurants from Swiggy Platform. Scrape Following\n",
        "things - Name, Area, Ratings, No of Ratings, Item Name, Item Category, Item Price,\n",
        "Item Description, Tags(Bestseller).\n"
      ],
      "metadata": {
        "id": "jOqhIqtE2D_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "links = [\"https://www.swiggy.com/restaurants/the-belgian-waffle-co-v-v-puram-shankarapura-bangalore-197238\",\n",
        "         \"https://www.swiggy.com/restaurants/burger-king-opposite-to-west-side-shivajinagar-bangalore-5938\",\n",
        "         \"https://www.swiggy.com/restaurants/polar-bear-vv-puram-shankarapura-bangalore-5893\"]\n",
        "frames = []         \n",
        "for link in links :\n",
        "  data_frame =  get_data(link)\n",
        "  frames.append(data_frame)  \n",
        "\n",
        "result = pd.concat(frames)        \n",
        "result.to_csv('menu.csv', index=False, encoding='utf-8')\n"
      ],
      "metadata": {
        "id": "uf0FcnN6ZgKF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data cleaning "
      ],
      "metadata": {
        "id": "scAXHilHfhU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = result\n",
        "df = df[pd.notnull(df['item_description'])]\n",
        "result = df"
      ],
      "metadata": {
        "id": "0DiIJGqLfgNI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_csv('menu.csv', index=False, encoding='utf-8')"
      ],
      "metadata": {
        "id": "QWOX0EragcvC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "ujsgbyBKiOYY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "STOP_WORDS = stopwords.words()\n",
        "\n",
        "# removing the emojies\n",
        "# https://www.kaggle.com/alankritamishra/covid-19-tweet-sentiment-analysis#Sentiment-analysis\n",
        "EMOJI_PATTERN = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "\n",
        "def cleaning(text):\n",
        "    \"\"\"\n",
        "    Convert to lowercase.\n",
        "    Rremove URL links, special characters and punctuation.\n",
        "    Tokenize and remove stop words.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('[’“”…]', '', text)\n",
        "\n",
        "    text = EMOJI_PATTERN.sub(r'', text)\n",
        "\n",
        "    # removing the stop-words\n",
        "    text_tokens = word_tokenize(text)\n",
        "    tokens_without_sw = [\n",
        "        word for word in text_tokens if not word in STOP_WORDS]\n",
        "    filtered_sentence = (\" \").join(tokens_without_sw)\n",
        "    text = filtered_sentence\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    max_rows = 1000  # 'None' to read whole file\n",
        "    input_file = 'menu.csv'\n",
        "    df = pd.read_csv(input_file,\n",
        "                     delimiter = ',',\n",
        "                     nrows = max_rows,\n",
        "                     engine = \"python\")\n",
        " \n",
        " ###### frequent words in Item Descrition ###########\n",
        "    dt = df['item_description'].apply(cleaning)\n",
        "    word_count = Counter(\" \".join(dt).split()).most_common(10)\n",
        "    print(\"###### frequent words in Item Descrition ###########\")\n",
        "    word_frequency = pd.DataFrame(word_count, columns = ['Word', 'Frequency'])\n",
        "    print(word_frequency)\n",
        "     \n",
        "###### frequent words in Item name ###########\n",
        "\n",
        "    dt = df['Food Item Name'].apply(cleaning)\n",
        "    word_count = Counter(\" \".join(dt).split()).most_common(10)\n",
        "    print(\"###### frequent words in Item Name ###########\")\n",
        "    word_frequency = pd.DataFrame(word_count, columns = ['Word', 'Frequency'])\n",
        "    print(word_frequency)\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv_FkFxEiHPM",
        "outputId": "3bd09638-ad37-4008-964f-fa6e13d2fa78"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "###### frequent words in Item Descrition ###########\n",
            "        Word  Frequency\n",
            "0  chocolate        256\n",
            "1     waffle        126\n",
            "2      cream        103\n",
            "3     crispy         99\n",
            "4          1         87\n",
            "5    chicken         87\n",
            "6      patty         84\n",
            "7        ice         80\n",
            "8      fries         73\n",
            "9    classic         69\n",
            "###### frequent words in Item Name ###########\n",
            "        Word  Frequency\n",
            "0    chicken         77\n",
            "1  chocolate         73\n",
            "2     waffle         65\n",
            "3      combo         55\n",
            "4          1         48\n",
            "5        veg         47\n",
            "6    whopper         46\n",
            "7      fries         34\n",
            "8     burger         31\n",
            "9        pcs         29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nke5IP85jztr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow this link for visualization\n",
        "https://datastudio.google.com/reporting/84def3d4-0c8e-4139-adf1-cdabb3050120"
      ],
      "metadata": {
        "id": "XCj6L0YTqtNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tLuxd6QpqyJ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}